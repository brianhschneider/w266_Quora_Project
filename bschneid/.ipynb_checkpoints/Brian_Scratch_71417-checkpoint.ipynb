{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import difflib\n",
    "import utils\n",
    "import math\n",
    "import re, string\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple baseline model...compare words in [Andrea's]\n",
    "\n",
    "def find_similarity(wl1, wl2):\n",
    "    # send 2 word lists to find matching sequence\n",
    "    sm = difflib.SequenceMatcher(None, wl1,wl2)\n",
    "    sm = sm.ratio()\n",
    "    return sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing, SM loop\n",
    "# code below runs through all IDs, comparing Q1 to Q2, storing similarity measurement [Andrea's]\n",
    "# I played around with a bunch of preprocessing improvements here, but none actually performed better than what we have currently, for log_loss\n",
    "\n",
    "sm_results = []\n",
    "\n",
    "for id in range(0, len(train)):\n",
    "# test with a smaller loop first\n",
    "# for id in range(0, 20):\n",
    "\n",
    "    q1 = str(train['question1'][id])\n",
    "    q2 = str(train['question2'][id])\n",
    "    \n",
    "    q1 = q1.translate(None, string.punctuation).lower()\n",
    "    q2 = q2.translate(None, string.punctuation).lower()\n",
    "\n",
    "    q1words = q1.split()\n",
    "    q2words = q2.split()\n",
    "    \n",
    "    sm_results.append([id, find_similarity(q1words, q2words)])\n",
    "# print(sm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.637853951947\n"
     ]
    }
   ],
   "source": [
    "actuals = np.array(train['is_duplicate'])\n",
    "n_sm_results = np.array(sm_results)\n",
    "predictions_sm = n_sm_results[:,1]\n",
    "score_sm = log_loss(actuals, predictions_sm)\n",
    "# accuracy_score_sm = accuracy_score(actuals, predictions_sm)\n",
    "# print(accuracy_score_sm)\n",
    "print(score_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I want to binarize here before running log loss.\n",
    "\n",
    "# sm_binarized_results = []\n",
    "\n",
    "# for i, t in sm_results:\n",
    "#     if t >= 0.75:\n",
    "#         sm_binarized_results.append([i, 1])\n",
    "#     else:\n",
    "#         sm_binarized_results.append([i, 0])\n",
    "\n",
    "# print(sm_binarized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Applied binarization, which netted us much worse results. Binarization will become more important for actual submissions, though.\n",
    "\n",
    "# actuals1 = np.array(train['is_duplicate'])\n",
    "# n_sm_results1 = np.array(sm_binarized_results)\n",
    "# predictions_sm1 = n_sm_results1[:,1]\n",
    "# score_sm1 = log_loss(actuals1, predictions_sm1)\n",
    "# print(score_sm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach below comes from https://github.com/ab-bh/Quora-Duplicate-Question-Pairs/blob/master/TF-IDF%20Approach%20.ipynb, which I wanted to try re: learning. I think the approach is pretty elegant re: setup and can be extended, edited, nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def clean_text(content):\n",
    "#     if type(content) == str:\n",
    "#         text = content.lower()\n",
    "# #         text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "# #         text = re.sub(\"[\"+string.punctuation+\"]\", \" \", text)\n",
    "#         text = content.translate(None, string.punctuation).lower()\n",
    "#         words=text.split()\n",
    "# #         stop_word=set(stopwords.words('english'))\n",
    "# #         words=list(word for word in words if not word in stop_word)\n",
    "# #         words=[word for word in words if len(word)>1 ]\n",
    "# #         words=[WordNetLemmatizer().lemmatize(word) for word in words]\n",
    "#         return ( \" \".join(words) )\n",
    "#     else:\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # this take a somewhat long time\n",
    "\n",
    "# train.question1 = train.question1.map(clean_text)\n",
    "# train.question2 = train.question2.map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Looking into TF-IDF -- this takes a really long time.\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer(analyzer='word', max_df=1.0, min_df=1)\n",
    "# tfidf_results = []\n",
    "    \n",
    "# for i in train.id:\n",
    "#     try:\n",
    "#         tfidf_matrix = tfidf_vectorizer.fit_transform([train.loc[i]['question1'], train.loc[i]['question2']])\n",
    "#         tfidf_results.append([i, round(cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0][1], 3)])\n",
    "#     except:\n",
    "#         tfidf_results.append([i, 0])\n",
    "# # print(tfidf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# actuals_tfidf = np.array(train['is_duplicate'])\n",
    "# n_tfidf_results = np.array(tfidf_results)\n",
    "# predictions_tfidf = n_tfidf_results[:,1]\n",
    "# score_tfidf = log_loss(actuals_tfidf, predictions_tfidf)\n",
    "# # accuracy_score_tfidf = accuracy_score(actuals_tfidf, predictions_tfidf)\n",
    "# # print(accuracy_score_tfidf)\n",
    "# print(score_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, N-grams. Bigrams and trigrams are both poor predictors pre-binarization, as they're mostly syntactical and not strong for semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'can'), ('can', 'i'), ('i', 'increase'), ('increase', 'the'), ('the', 'speed'), ('speed', 'of'), ('of', 'my'), ('my', 'internet'), ('internet', 'connection'), ('connection', 'while'), ('while', 'using'), ('using', 'a'), ('a', 'vpn')]\n",
      "[('how', 'can'), ('can', 'internet'), ('internet', 'speed'), ('speed', 'be'), ('be', 'increased'), ('increased', 'by'), ('by', 'hacking'), ('hacking', 'through'), ('through', 'dns')]\n"
     ]
    }
   ],
   "source": [
    "# Taking bi-grams for each question.\n",
    "# Must tokenize before starting.\n",
    "\n",
    "id=2\n",
    "# bigram1 = list(bigrams(str(train['question1'][id])))\n",
    "# bigram2 = list(bigrams(str(train['question2'][id])))\n",
    "\n",
    "q1 = str(train['question1'][id])\n",
    "q2 = str(train['question2'][id])\n",
    "    \n",
    "q1 = q1.translate(None, string.punctuation).lower()\n",
    "q2 = q2.translate(None, string.punctuation).lower()\n",
    "\n",
    "q1words = q1.split()\n",
    "q2words = q2.split()\n",
    "\n",
    "bigram1 = list(bigrams(q1words))\n",
    "bigram2 = list(bigrams(q2words))\n",
    "\n",
    "print bigram1\n",
    "print bigram2\n",
    "\n",
    "# # list(bigrams(['more', 'is', 'said', 'than', 'done']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def bigrammer(words):\n",
    "#     return zip(words, words[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create loop to go through for each id and come up w/ a percentage of bigram overlap\n",
    "# Removing stopwords increased log_loss quite a bit.\n",
    "\n",
    "bigram_similarity_results = []\n",
    "\n",
    "for id in range(0, len(train)):\n",
    "# test with a smaller loop first\n",
    "# for id in range(0, 20):\n",
    "\n",
    "    q1 = str(train['question1'][id])\n",
    "    q2 = str(train['question2'][id])\n",
    "    \n",
    "    q1 = q1.translate(None, string.punctuation).lower()\n",
    "    q2 = q2.translate(None, string.punctuation).lower()\n",
    "\n",
    "    q1words = q1.split()\n",
    "    q2words = q2.split()\n",
    "    \n",
    "#     stop_word=set(stopwords.words('english'))\n",
    "#     q1words=list(word for word in q1words if not word in stop_word)\n",
    "#     q2words=list(word for word in q2words if not word in stop_word)\n",
    "    \n",
    "    bigram1 = list(bigrams(q1words))\n",
    "    bigram2 = list(bigrams(q2words))\n",
    "    \n",
    "#     print bigram1, bigram2\n",
    "    \n",
    "    bigram_similarity_results.append([id, find_similarity(bigram1, bigram2)])\n",
    "    \n",
    "# print(bigram_similarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.85594647997\n"
     ]
    }
   ],
   "source": [
    "# bigram log_loss\n",
    "\n",
    "actuals = np.array(train['is_duplicate'])\n",
    "n_bigram_results = np.array(bigram_similarity_results)\n",
    "predictions_bigram = n_bigram_results[:,1]\n",
    "score_bigram = log_loss(actuals, predictions_bigram)\n",
    "# accuracy_score_bigram = accuracy_score(actuals, predictions_bigram)\n",
    "# print(accuracy_score_bigram)\n",
    "print(score_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# trigrams\n",
    "# removing stopwords was not at all effective in lowering log_loss for ngram models\n",
    "\n",
    "trigram_similarity_results = []\n",
    "\n",
    "for id in range(0, len(train)):\n",
    "# test with a smaller loop first\n",
    "# for id in range(0, 20):\n",
    "\n",
    "    q1 = str(train['question1'][id])\n",
    "    q2 = str(train['question2'][id])\n",
    "    \n",
    "    q1 = q1.translate(None, string.punctuation).lower()\n",
    "    q2 = q2.translate(None, string.punctuation).lower()\n",
    "\n",
    "    q1words = q1.split()\n",
    "    q2words = q2.split()\n",
    "    \n",
    "#     stop_word=set(stopwords.words('english'))\n",
    "#     q1words=list(word for word in q1words if not word in stop_word)\n",
    "#     q2words=list(word for word in q2words if not word in stop_word)    \n",
    "    \n",
    "    trigram1 = list(trigrams(q1words))\n",
    "    trigram2 = list(trigrams(q2words))\n",
    "    \n",
    "#     print trigram1, trigram2\n",
    "    \n",
    "    trigram_similarity_results.append([id, find_similarity(trigram1, trigram2)])\n",
    "\n",
    "# print(trigram_similarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.29866130866\n"
     ]
    }
   ],
   "source": [
    "# trigram log_loss\n",
    "\n",
    "actuals = np.array(train['is_duplicate'])\n",
    "n_trigram_results = np.array(trigram_similarity_results)\n",
    "predictions_trigram = n_trigram_results[:,1]\n",
    "score_trigram = log_loss(actuals, predictions_trigram)\n",
    "# accuracy_score_trigram = accuracy_score(actuals, predictions_trigram)\n",
    "# print(accuracy_score_sm)\n",
    "print(score_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# character bi-gram similarity\n",
    "\n",
    "char_bi_similarity_results = []\n",
    "\n",
    "for id in range(0, len(train)):\n",
    "# test with a smaller loop first\n",
    "# for id in range(0, 20):\n",
    "\n",
    "    q1 = str(train['question1'][id])\n",
    "    q2 = str(train['question2'][id])\n",
    "    \n",
    "    q1 = q1.translate(None, string.punctuation).lower()\n",
    "    q2 = q2.translate(None, string.punctuation).lower()\n",
    "\n",
    "#     q1words = q1.split()\n",
    "#     q2words = q2.split()\n",
    "    \n",
    "#     stop_word=set(stopwords.words('english'))\n",
    "#     q1words=list(word for word in q1words if not word in stop_word)\n",
    "#     q2words=list(word for word in q2words if not word in stop_word)\n",
    "    \n",
    "    char_bigram1 = list(bigrams(q1))\n",
    "    char_bigram2 = list(bigrams(q2))\n",
    "    \n",
    "#     print bigram1, bigram2\n",
    "    \n",
    "    char_bi_similarity_results.append([id, find_similarity(char_bigram1, char_bigram2)])\n",
    "    \n",
    "# print(char_tri_similarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.650028562634\n"
     ]
    }
   ],
   "source": [
    "# character bigram log_loss\n",
    "\n",
    "actuals = np.array(train['is_duplicate'])\n",
    "n_char_bigram_results = np.array(char_bi_similarity_results)\n",
    "predictions_char_bigram = n_char_bigram_results[:,1]\n",
    "score_char_bigram = log_loss(actuals, predictions_char_bigram)\n",
    "# accuracy_score_trigram = accuracy_score(actuals, predictions_trigram)\n",
    "# print(accuracy_score_sm)\n",
    "print(score_char_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# character tri-gram similarity\n",
    "\n",
    "char_tri_similarity_results = []\n",
    "\n",
    "for id in range(0, len(train)):\n",
    "# test with a smaller loop first\n",
    "# for id in range(0, 20):\n",
    "\n",
    "    q1 = str(train['question1'][id])\n",
    "    q2 = str(train['question2'][id])\n",
    "    \n",
    "    q1 = q1.translate(None, string.punctuation).lower()\n",
    "    q2 = q2.translate(None, string.punctuation).lower()\n",
    "\n",
    "#     q1words = q1.split()\n",
    "#     q2words = q2.split()\n",
    "    \n",
    "#     stop_word=set(stopwords.words('english'))\n",
    "#     q1words=list(word for word in q1words if not word in stop_word)\n",
    "#     q2words=list(word for word in q2words if not word in stop_word)\n",
    "    \n",
    "    char_trigram1 = list(trigrams(q1))\n",
    "    char_trigram2 = list(trigrams(q2))\n",
    "    \n",
    "#     print bigram1, bigram2\n",
    "    \n",
    "    char_tri_similarity_results.append([id, find_similarity(char_trigram1, char_trigram2)])\n",
    "    \n",
    "# print(char_tri_similarity_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620707028792\n"
     ]
    }
   ],
   "source": [
    "# character trigram log_loss\n",
    "\n",
    "actuals = np.array(train['is_duplicate'])\n",
    "n_char_trigram_results = np.array(char_tri_similarity_results)\n",
    "predictions_char_trigram = n_char_trigram_results[:,1]\n",
    "score_char_trigram = log_loss(actuals, predictions_char_trigram)\n",
    "# accuracy_score_trigram = accuracy_score(actuals, predictions_trigram)\n",
    "# print(accuracy_score_sm)\n",
    "print(score_char_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I want to add some simple features that will likely be beneficial\n",
    "\n",
    "def common_words(x):\n",
    "    q1, q2 = x\n",
    "    return len(set(str(q1).lower().split()) & set(str(q2).lower().split()))\n",
    "\n",
    "def words_count(question):\n",
    "    return len(str(question).split())\n",
    "\n",
    "def length(question):\n",
    "    return len(str(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    return word.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I want to look into LSI and LDA similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding features to train\n",
    "\n",
    "train['trigram-word-similarity'] = predictions_trigram\n",
    "train['bigram-word-similarity'] = predictions_bigram\n",
    "train['trigram-char-similarity'] = predictions_char_trigram\n",
    "train['bigram-char-similarity'] = predictions_char_bigram\n",
    "train['q1_num_words'] = train['question1'].map(words_count)\n",
    "train['q2_numwords'] = train['question2'].map(words_count)\n",
    "train['q1_sent_length'] = train['question1'].map(length)\n",
    "train['q2_sent_length'] = train['question2'].map(length)\n",
    "train['common_words'] = train[['question1', 'question2']].apply(common_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>trigram-word-similarity</th>\n",
       "      <th>bigram-word-similarity</th>\n",
       "      <th>trigram-char-similarity</th>\n",
       "      <th>bigram-char-similarity</th>\n",
       "      <th>q1_words_num</th>\n",
       "      <th>q2_words_num</th>\n",
       "      <th>q1_length</th>\n",
       "      <th>q2_length</th>\n",
       "      <th>common_words</th>\n",
       "      <th>q1_num_words</th>\n",
       "      <th>q2_numwords</th>\n",
       "      <th>q1_sent_length</th>\n",
       "      <th>q2_sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.924370</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039604</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224299</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "   trigram-word-similarity  bigram-word-similarity  trigram-char-similarity  \\\n",
       "0                 0.909091                0.916667                 0.923077   \n",
       "1                 0.117647                0.210526                 0.512000   \n",
       "2                 0.000000                0.090909                 0.269841   \n",
       "3                 0.000000                0.000000                 0.000000   \n",
       "4                 0.000000                0.000000                 0.224299   \n",
       "\n",
       "   bigram-char-similarity  q1_words_num  q2_words_num  q1_length  q2_length  \\\n",
       "0                0.924370            14            12         66         57   \n",
       "1                0.582677             8            13         51         88   \n",
       "2                0.312500            14            10         73         59   \n",
       "3                0.039604            11             9         50         65   \n",
       "4                0.293578            13             7         76         39   \n",
       "\n",
       "   common_words  q1_num_words  q2_numwords  q1_sent_length  q2_sent_length  \n",
       "0            10            14           12              66              57  \n",
       "1             4             8           13              51              88  \n",
       "2             4            14           10              73              59  \n",
       "3             0            11            9              50              65  \n",
       "4             2            13            7              76              39  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run random forest at the end\n",
    "\n",
    "train, test = train_test_split(train, test_size = 0.2)\n",
    "\n",
    "Y = train.is_duplicate\n",
    "X = train[['trigram-word-similarity', 'bigram-word-similarity', 'trigram-char-similarity', 'bigram-char-similarity', 'q1_words_num', 'q2_words_num', 'q1_length', 'q2_length', 'common_words', 'q1_num_words', 'q2_numwords', 'q1_sent_length', 'q2_sent_length']]\n",
    "Y1 = test.is_duplicate\n",
    "X1 = test[['trigram-word-similarity', 'bigram-word-similarity', 'trigram-char-similarity', 'bigram-char-similarity', 'q1_words_num', 'q2_words_num', 'q1_length', 'q2_length', 'common_words', 'q1_num_words', 'q2_numwords', 'q1_sent_length', 'q2_sent_length']]\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=2)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "preds = clf.predict(X1)\n",
    "# clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trigram-word-similarity', 0.052654594077853292),\n",
       " ('bigram-word-similarity', 0.085351440061041728),\n",
       " ('trigram-char-similarity', 0.1841751874576136),\n",
       " ('bigram-char-similarity', 0.16940546297003919),\n",
       " ('q1_words_num', 0.029112562437234028),\n",
       " ('q2_words_num', 0.031861099535370419),\n",
       " ('q1_length', 0.073100233361969164),\n",
       " ('q2_length', 0.077853735173448291),\n",
       " ('common_words', 0.080629270199144121),\n",
       " ('q1_num_words', 0.030288919563345635),\n",
       " ('q2_numwords', 0.035607484999337538),\n",
       " ('q1_sent_length', 0.073116444640596689),\n",
       " ('q2_sent_length', 0.076843565523006377)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X, clf.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.93879567684\n"
     ]
    }
   ],
   "source": [
    "actuals = np.array(Y1)\n",
    "score_rfc = log_loss(actuals, preds)\n",
    "# accuracy_score = accuracy_score(actuals, preds)\n",
    "# print(accuracy_score)\n",
    "print(score_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
