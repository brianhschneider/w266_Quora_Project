{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-LSTM Approach 3\n",
    "\n",
    "Based on approach from class, adjusted for multiple inputs\n",
    "\n",
    "This approach was abandoned when team decided to concentrate on CNN model (and various variations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "import string          # various string functions\n",
    "import difflib         # classes and functions for comparing sequences\n",
    "import utils           # word processing functions and distance functions, pretty printing and data loading\n",
    "\n",
    "from importlib import reload       # reload external files\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# rnn code\n",
    "import rnnlstm\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import vocabulary, tf_embed_viz\n",
    "\n",
    "# set a default vocab size\n",
    "V=95596\n",
    "MAX_WORDS = 200000\n",
    "EMBED_DIM = 300\n",
    "MAX_SEQUENCE = 25\n",
    "\n",
    "import plotly.plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# need to look into plotly issues with this line in python3\n",
    "# plotly.offline.init_notebook_mode() # run at the start of every ipython notebook\n",
    "\n",
    "from sklearn.metrics import log_loss    # used in measurement / scoring\n",
    "from sklearn.metrics import classification_report   # among other things, provides accuracy and f1\n",
    "\n",
    "# Your code\n",
    "# import rnnlm\n",
    "# import rnnlm_test\n",
    "# reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate training data dataframe\n",
    "# train_lite for testing approach\n",
    "# train = pd.read_csv('Data/train_lite.csv') #index_col=0\n",
    "\n",
    "# full train for when working\n",
    "train = pd.read_csv('Data/train.csv') #index_col=0\n",
    "\n",
    "print (len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(questions, V=10000):\n",
    "    #token_feed = (canonicalize_word(w) for w in sentence.split() for sentence in questions)\n",
    "    #token_feed = (canonicalize_word(w) for sentence in questions for w in sentence.split())\n",
    "    #print (token_feed)\n",
    "    \n",
    "    token_feed = []\n",
    "    for sentence in questions:\n",
    "        for w in str(sentence).split():\n",
    "            token_feed.append(utils.canonicalize_word(w))\n",
    "    token_feed = set(token_feed)\n",
    "    \n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(vocabulary)\n",
    "reload(utils)\n",
    "question1 = train.question1.tolist()\n",
    "question2 = train.question2.tolist()\n",
    "is_duplicate = train.is_duplicate.tolist()\n",
    "\n",
    "# print('Question pairs: %d' % len(question1))\n",
    "\n",
    "# build word index\n",
    "questions = question1 + question2\n",
    "\n",
    "vocab = build_vocab(questions, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(vocab.ordered_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = vocab.ordered_words()\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab.word_to_id.get('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# critical definition\n",
    "# maps all words to ID\n",
    "\n",
    "wordset =  vocab.word_to_id\n",
    "print (len(wordset))\n",
    "print (type(wordset))\n",
    "print (wordset.get('pay'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sent_old(q, max_time):\n",
    "    if len(q) > max_time:\n",
    "        # cut it off\n",
    "        q = q[0:max_time]\n",
    "    else:\n",
    "        # pad to max_time\n",
    "        amt_to_pad = max_time - len(q)\n",
    "        q.extend([2] * amt_to_pad)\n",
    "    return q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad sentences to make equivalent length\n",
    "def pad_sent(q):\n",
    "    if len(q) > MAX_SEQUENCE:\n",
    "        # cut it off\n",
    "        q = q[0:MAX_SEQUENCE]\n",
    "    else:\n",
    "        # pad to max_time\n",
    "        amt_to_pad = MAX_SEQUENCE - len(q)\n",
    "        q.extend([2] * amt_to_pad)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_questions(questions, wordset, vocab):\n",
    "    sents = [\"<s>\" + str(s) + \"</s>\" for s in questions]\n",
    "    # print (sents[0:5])\n",
    " \n",
    "    sequence = []\n",
    "    num_sequence = []\n",
    "    for s in str(sents):\n",
    "        # for each sentence in list of sentences\n",
    "        # print (s)\n",
    "\n",
    "        s_words = []\n",
    "        s_w_ids = []\n",
    "        counter = 0\n",
    "        for w in str(s).split():\n",
    "            # capture list of words for sentence\n",
    "            # once all captured, add to list with sequence\n",
    "            \n",
    "            # for each word in split sentence\n",
    "            # print (w)\n",
    "            \n",
    "            # only need to do this for max_sequence counts\n",
    "            if counter < MAX_SEQUENCE:\n",
    "                w = w.lower()\n",
    "                if (wordset == None) or (w in wordset): \n",
    "                    s_words.append(w)\n",
    "                    s_w_ids.append(vocab.word_to_id.get(w))\n",
    "                else:\n",
    "                    s_words.append(\"<unk>\") # unknown token\n",
    "                    s_w_ids.append(vocab.word_to_id.get(\"<unk>\"))\n",
    "                               \n",
    "                '''\n",
    "                if w.isdigit():\n",
    "                    if (wordset != None) and (w in wordset):\n",
    "                        q1_sequence.append(w)\n",
    "\n",
    "                    w = utils.canonicalize_digits(w) # try to canonicalize numbers\n",
    "                '''\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "        # pad or cut to 25 words:\n",
    "        s_words = pad_sent([s_words])\n",
    "        s_w_ids = pad_sent([s_w_ids])\n",
    "        \n",
    "        sequence.append(s_words)\n",
    "        num_sequence.append(s_w_ids)\n",
    "        \n",
    "        \n",
    "    sequence = np.array(sequence)\n",
    "    num_sequence = np.array(num_sequence)\n",
    "    return sequence, num_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_sequences, q1_seq_num = process_questions(question1, wordset, vocab)\n",
    "q2_sequences, q2_seq_num = process_questions(question2, wordset, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (q1_sequences.shape)\n",
    "print (q2_sequences.shape)\n",
    "\n",
    "# NOTE, with sequences, each original sentence is a list within the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload (rnnlstm)\n",
    "\n",
    "# building graph based on a3 assigment\n",
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "# V, H, and num_layers are all things we can play with...\n",
    "# default corpus is 'brown'\n",
    "# for now, want to see running with this data\n",
    "# set hyperparameters\n",
    "lm = rnnlstm.RNNLM(V=V, H=200, num_layers=2)\n",
    "\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = np.stack((q1_sequences, q2_sequences), axis=1)\n",
    "X = np.stack((q1_seq_num, q2_seq_num), axis=1)\n",
    "\n",
    "#y = is_duplicate\n",
    "y = np.array(is_duplicate)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max_time = 100\n",
    "\n",
    "# not sure these matter any more:\n",
    "max_time = 50\n",
    "# sentences with > 50 words will be truncated to 50 words\n",
    "# rest are padded to 50\n",
    "\n",
    "# for our purposes, a batch is a question pair\n",
    "# this varies whether train or test though...\n",
    "# something to think about\n",
    "#batch_size = len(Q1_train)\n",
    "batch_size = 1\n",
    "print (batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(q1, q2, label, max_time):\n",
    "    for i in range(len(q1)):\n",
    "        # # of q1 and q2 questions is the same, so can use one to iterate\n",
    "        \n",
    "        # padded or trunctating all sentences to max_time\n",
    "        q1[i] = pad_sent(q1[i], max_time)\n",
    "        q2[i] = pad_sent(q2[i], max_time)\n",
    "        \n",
    "    # now we will make them 1 list (vs 2)\n",
    "    q1 = np.array([idx for sublist in q1 for idx in sublist] )\n",
    "    q2 = np.array([idx for sublist in q2 for idx in sublist] )\n",
    "    \n",
    "    # print (type(q1))\n",
    "    # print (q1.shape)\n",
    "        \n",
    "    q1 = q1.reshape([batch_size,-1])\n",
    "    q2 = q2.reshape([batch_size,-1])\n",
    "    label = label.reshape([batch_size, -1])\n",
    "    \n",
    "    print (q1.shape)\n",
    "    print (q1.shape[1])\n",
    "    \n",
    "    # need to look at label, why is it 9000???\n",
    "    \n",
    "    print (label.shape)\n",
    "    \n",
    "    # Yield batches\n",
    "    # yield q1[i], q2[i], label[i]\n",
    "    # return q1, q2, label\n",
    "\n",
    "    # Yield batches\n",
    "    for i in range(0, q1.shape[1], max_time):\n",
    "        yield q1[:,i:i+max_time], q2[:,i:i+max_time], label[:,i:i+max_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# possible add another for loop, that allows you to move a sentence over multiple\n",
    "# need to look at what batch_generator was providing in a3 (perhaps on the cloud even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing batch generator\n",
    "bi = batch_generator(Q1_train, Q2_train, y_train, max_time)\n",
    "\n",
    "\n",
    "for i, (w1, w2, y) in enumerate(bi):\n",
    "    print (\"i:\", i)\n",
    "    print (\"w1:\", w1)\n",
    "    print (\"w2:\", w2)\n",
    "    print (\"len(w1):\", len(w1))\n",
    "    print (\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt = [[[2,3,4,5]], [[4,5,6,7]], [[2,2,2,2]]]\n",
    "npt = np.asarray(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "learning_rate = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlstm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlstm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1 = [2, 4660, 2, 555, 7902, 2, 2, 1804, 1908, 10320, 283, 2, 4842, 2]\n",
    "#q1.reshape([batch_size,-1])\n",
    "for i in range(0, len(q1), max_time):\n",
    "    print (i)\n",
    "    print (type(q1))\n",
    "    print (type(max_time))\n",
    "    #print(q1[:,i:i+max_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        \n",
    "    # How do I bring in both w's? Getting a \"need to pass W_1 through cost, _, h\" function. No idea.\n",
    "\n",
    "    for i, (w1, w2, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean initial state.\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        if i == 0:\n",
    "            #print (\"lm.initial_h_: \", lm.initial_h_)\n",
    "            #print (\"here\")\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_q1_: w1, lm.input_w_q2_: w2})\n",
    "            #print (\"here1\")\n",
    "            \n",
    "\n",
    "        \n",
    "        feed_dict = {lm.input_w_q1_: w1, lm.input_w_q2_: w2, lm.target_y_: y, lm.initial_h_: h}\n",
    "        #print (\"w1 shape: \", np.shape(w1))\n",
    "        #print (\"w2 shape: \", np.shape(w2))\n",
    "        #print (\"y shape: \", np.shape(y))\n",
    "        #print (\"h shape: \", np.shape(h))\n",
    "        \n",
    "        #print(\"lm.train_loss_:\", lm.train_loss_)\n",
    "        \n",
    "        #cost, _, h = session.run([lm.train_loss_, lm.train_step_, lm.final_h_], feed_dict=feed_dict)\n",
    "        \n",
    "        train_step, cost, h = session.run([train_op, loss, lm.final_h_],feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += batch_size * max_time\n",
    "\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" %\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# passing both questions at once\n",
    "reload(utils)\n",
    "def score_dataset(lm, session, Q1, Q2, y, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    # bi = utils.batch_generator(Q1, Q2, y, batch_size=50, max_time=100)\n",
    "    bi = batch_generator(Q1, Q2, y, max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload (rnnlstm)\n",
    "reload (utils)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlstm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        #bi = utils.batch_generator(Q1_train, Q2_train, y_train, batch_size, max_time)\n",
    "        bi = batch_generator(Q1_train, Q2_train, y_train, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi, train=True) \n",
    "\n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, Q1_test, Q2_test, y_test, name=\"Test set\")\n",
    "        print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
