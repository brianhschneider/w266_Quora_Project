{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project, Attempting to get RNN LSTM to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "import string          # various string functions\n",
    "import difflib         # classes and functions for comparing sequences\n",
    "import utils           # word processing functions and distance functions, pretty printing and data loading\n",
    "\n",
    "from importlib import reload       # reload external files\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# rnn code\n",
    "import rnnlstm\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import vocabulary, tf_embed_viz\n",
    "\n",
    "# set a default vocab size\n",
    "V=20000\n",
    "\n",
    "import plotly.plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# need to look into plotly issues with this line in python3\n",
    "# plotly.offline.init_notebook_mode() # run at the start of every ipython notebook\n",
    "\n",
    "from sklearn.metrics import log_loss    # used in measurement / scoring\n",
    "from sklearn.metrics import classification_report   # among other things, provides accuracy and f1\n",
    "\n",
    "# Your code\n",
    "# import rnnlm\n",
    "# import rnnlm_test\n",
    "# reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Generate training data dataframe\n",
    "# train_lite for testing approach\n",
    "train = pd.read_csv('Data/train_lite.csv') #index_col=0\n",
    "# full train for when working\n",
    "# train = pd.read_csv('Data/train.csv') #index_col=0\n",
    "\n",
    "print (len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57674</td>\n",
       "      <td>5725</td>\n",
       "      <td>38477</td>\n",
       "      <td>How can I get a list of my Gmail accounts?</td>\n",
       "      <td>How do I find my list of GMail addresses?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185568</td>\n",
       "      <td>283254</td>\n",
       "      <td>283255</td>\n",
       "      <td>What are the symptoms of child abuse?</td>\n",
       "      <td>What are signs of child abuse?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>359111</td>\n",
       "      <td>400936</td>\n",
       "      <td>488715</td>\n",
       "      <td>What are some examples of selfishness?</td>\n",
       "      <td>Do we all occasionally tend to emotional masoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204216</td>\n",
       "      <td>281934</td>\n",
       "      <td>307050</td>\n",
       "      <td>Is Qnet a scam?</td>\n",
       "      <td>Where is the registered office of Qnet in Mumbai?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5649</td>\n",
       "      <td>11103</td>\n",
       "      <td>11104</td>\n",
       "      <td>What do Americans think about Donald Trump?</td>\n",
       "      <td>What do you think about Donald Trump pick?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                    question1  \\\n",
       "0   57674    5725   38477   How can I get a list of my Gmail accounts?   \n",
       "1  185568  283254  283255        What are the symptoms of child abuse?   \n",
       "2  359111  400936  488715       What are some examples of selfishness?   \n",
       "3  204216  281934  307050                              Is Qnet a scam?   \n",
       "4    5649   11103   11104  What do Americans think about Donald Trump?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0          How do I find my list of GMail addresses?             1  \n",
       "1                     What are signs of child abuse?             1  \n",
       "2  Do we all occasionally tend to emotional masoc...             0  \n",
       "3  Where is the registered office of Qnet in Mumbai?             0  \n",
       "4         What do you think about Donald Trump pick?             1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train['Unnamed: 0']\n",
    "del train['Unnamed: 0.1']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(vocabulary)\n",
    "reload(utils)\n",
    "question1 = train.question1.tolist()\n",
    "question2 = train.question2.tolist()\n",
    "is_duplicate = train.is_duplicate.tolist()\n",
    "\n",
    "# print('Question pairs: %d' % len(question1))\n",
    "\n",
    "# build word index\n",
    "questions = question1 + question2\n",
    "\n",
    "vocab = utils.build_vocab(questions, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(vocab.ordered_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 20000\n"
     ]
    }
   ],
   "source": [
    "word_index = vocab.ordered_words()\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word_to_id.get('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "<class 'dict'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# critical definition\n",
    "# maps all words to ID\n",
    "\n",
    "wordset =  vocab.word_to_id\n",
    "print (len(wordset))\n",
    "print (type(wordset))\n",
    "print (wordset.get('paying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_questions(questions, wordset, vocab):\n",
    "    sents = [\"<s>\" + s + \"</s>\" for s in questions]\n",
    "    # print (sents[0:5])\n",
    " \n",
    "    sequence = []\n",
    "    num_sequence = []\n",
    "    for s in sents:\n",
    "        # for each sentence in list of sentences\n",
    "        # print (s)\n",
    "        s_words = []\n",
    "        s_w_ids = []\n",
    "        for w in s.split():\n",
    "            # capture list of words for sentence\n",
    "            # once all captured, add to list with sequence\n",
    "            \n",
    "            # for each word in split sentence\n",
    "            # print (w)\n",
    "            w = w.lower()\n",
    "            if (wordset == None) or (w in wordset): \n",
    "                s_words.append(w)\n",
    "                s_w_ids.append(vocab.word_to_id.get(w))\n",
    "            else:\n",
    "                s_words.append(\"<unk>\") # unknown token\n",
    "                s_w_ids.append(vocab.word_to_id.get(\"<unk>\"))\n",
    "                               \n",
    "            '''\n",
    "            if w.isdigit():\n",
    "            if (wordset != None) and (w in wordset):\n",
    "                    q1_sequence.append(w)\n",
    "\n",
    "                w = utils.canonicalize_digits(w) # try to canonicalize numbers\n",
    "            '''\n",
    "\n",
    "        sequence.append(s_words)\n",
    "        num_sequence.append(s_w_ids)\n",
    "        \n",
    "        \n",
    "    sequence = np.array(sequence)\n",
    "    num_sequence = np.array(num_sequence)\n",
    "    return sequence, num_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_sequences, q1_seq_num = process_questions(question1, wordset, vocab)\n",
    "q2_sequences, q2_seq_num = process_questions(question2, wordset, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ ['<unk>', 'can', '<unk>', 'get', 'a', 'list', '<unk>', 'my', '<unk>', '<unk>'],\n",
       "       ['<unk>', 'are', 'the', 'symptoms', '<unk>', 'child', '<unk>'],\n",
       "       ['<unk>', 'are', 'some', 'examples', '<unk>', '<unk>'], ...,\n",
       "       ['<unk>', 'does', 'gases', 'heat', 'up', 'if', 'we', 'try', 'to', 'cool', 'it', '<unk>', 'expanding', 'it', 'above', 'inversion', '<unk>'],\n",
       "       ['<unk>', 'missed', 'the', 'call', 'on', 'brexit;', 'how', 'accurate', 'are', 'current', 'polling', 'methods', 'and', 'could', 'we', 'see', '<unk>', 'forecasting', 'flubs', 'with', 'the', 'presidential', '<unk>'],\n",
       "       ['<unk>', 'can', 'be', '<unk>', 'about', 'the', 'hyperinflation', '<unk>', 'zimbabwe?', 'what', 'were', 'the', 'causes', '<unk>', 'it', 'and', 'its', 'impact', 'on', 'the', 'economy', '<unk>', 'the', '<unk>']], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 4535, 1088, 18597, 2, 2, 2],\n",
       "       [2, 10517, 4520, 2, 15722, 2],\n",
       "       [2, 11222, 12316, 11923, 13401, 18943, 5987, 2], ...,\n",
       "       [2, 4136, 17114, 653, 13653, 2206, 2, 2, 14510, 15509, 16553, 2],\n",
       "       [2, 6364, 12210, 8754, 13427, 14284, 9808, 18943, 18191, 7580, 1603, 8086, 1410, 3557, 10621, 14510, 7580, 3402, 19665, 15509, 16553, 408, 12772, 4666, 10096, 19465, 13494, 905, 1410, 5033, 3665, 2],\n",
       "       [2, 2, 1323, 4585, 2, 7580, 12838, 12097, 19665, 2]], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_seq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len(q1_sequences))\n",
    "print (len(q2_sequences))\n",
    "\n",
    "# NOTE, with sequences, each original sentence is a list within the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload (rnnlstm)\n",
    "\n",
    "# building graph based on a3 assigment\n",
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "# V, H, and num_layers are all things we can play with...\n",
    "# default corpus is 'brown'\n",
    "# for now, want to see running with this data\n",
    "# set hyperparameters\n",
    "lm = rnnlstm.RNNLM(V=V, H=200, num_layers=2)\n",
    "\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = np.stack((q1_sequences, q2_sequences), axis=1)\n",
    "X = np.stack((q1_seq_num, q2_seq_num), axis=1)\n",
    "\n",
    "#y = is_duplicate\n",
    "y = np.array(is_duplicate)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10517, 13653, 2, 13494, 1005, 15553, 4010, 2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    }
   ],
   "source": [
    "# max_time = 100\n",
    "\n",
    "# not sure these matter any more:\n",
    "max_time = 50\n",
    "# sentences with > 50 words will be truncated to 50 words\n",
    "# rest are padded to 50\n",
    "\n",
    "# for our purposes, a batch is a question pair\n",
    "# this varies wether train or test though...\n",
    "# something to think about\n",
    "batch_size = len(Q1_train)\n",
    "print (batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sent(q, max_time):\n",
    "    if len(q) > max_time:\n",
    "        # cut it off\n",
    "        q = q[0:max_time]\n",
    "    else:\n",
    "        # pad to max_time\n",
    "        amt_to_pad = max_time - len(q)\n",
    "        q.extend([2] * amt_to_pad)\n",
    "    return q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(q1, q2, label, max_time):\n",
    "    for i in range(len(q1)):\n",
    "        # # of q1 and q2 questions is the same, so can use one to iterate\n",
    "        \n",
    "        # padded or trunctating all sentences to max_time\n",
    "        q1[i] = pad_sent(q1[i], max_time)\n",
    "        q2[i] = pad_sent(q2[i], max_time)\n",
    "        \n",
    "    # now we will make them 1 list (vs 2)\n",
    "    q1 = np.array([idx for sublist in q1 for idx in sublist] )\n",
    "    q2 = np.array([idx for sublist in q2 for idx in sublist] )\n",
    "    \n",
    "    # print (type(q1))\n",
    "    # print (q1.shape)\n",
    "        \n",
    "    q1 = q1.reshape([batch_size,-1])\n",
    "    q2 = q2.reshape([batch_size,-1])\n",
    "    label = label.reshape([batch_size, -1])\n",
    "    \n",
    "    # print (q1.shape)\n",
    "    # print (q1.shape[1])\n",
    "    \n",
    "    # need to look at label, why is it 9000???\n",
    "    \n",
    "    #print (label.shape)\n",
    "    # Yield batches\n",
    "    # yield q1[i], q2[i], label[i]\n",
    "    # return q1, q2, label\n",
    "\n",
    "    # Yield batches\n",
    "    for i in range(0, q1.shape[1], max_time):\n",
    "        yield q1[:,i:i+max_time], q2[:,i:i+max_time], label[:,i:i+max_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# possible add another for loop, that allows you to move a sentence over multiple\n",
    "# need to look at what batch_generator was providing in a3 (perhaps on the cloud even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<class 'list'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "q1 = [2, 4660, 2, 555, 7902, 2, 2, 1804, 1908, 10320, 283, 2, 4842, 2]\n",
    "#q1.reshape([batch_size,-1])\n",
    "for i in range(0, len(q1), max_time):\n",
    "    print (i)\n",
    "    print (type(q1))\n",
    "    print (type(max_time))\n",
    "    #print(q1[:,i:i+max_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "w1: [[    2 10517 13653 ...,     2     2     2]\n",
      " [    2 13605  7580 ...,     2     2     2]\n",
      " [    2 13605 13494 ...,     2     2     2]\n",
      " ..., \n",
      " [    2 13605 13494 ...,     2     2     2]\n",
      " [    2 19043     2 ...,     2     2     2]\n",
      " [    2 13605 19763 ...,     2     2     2]]\n",
      "w2: [[    2 13605 13494 ...,     2     2     2]\n",
      " [    2 13494  1005 ...,     2     2     2]\n",
      " [    2 13605 13494 ...,     2     2     2]\n",
      " ..., \n",
      " [    2   164 13494 ...,     2     2     2]\n",
      " [    2 19043 12772 ...,     2     2     2]\n",
      " [    2 13605 13494 ...,     2     2     2]]\n",
      "y: [[1]\n",
      " [0]\n",
      " [1]\n",
      " ..., \n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# testing batch generator\n",
    "bi = batch_generator(Q1_train, Q2_train, y_train, max_time)\n",
    "for i, (w1, w2, y) in enumerate(bi):\n",
    "    print (\"i:\", i)\n",
    "    print (\"w1:\", w1)\n",
    "    print (\"w2:\", w2)\n",
    "    print (\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        \n",
    "    # How do I bring in both w's? Getting a \"need to pass W_1 through cost, _, h\" function. No idea.\n",
    "\n",
    "    for i, (w1, w2, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean initial state.\n",
    "\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_q1_: w1, lm.input_w_q2_: w2})\n",
    "\n",
    "       \n",
    "        feed_dict = {lm.input_w_q1_: w1, lm.input_w_q2_: w2, lm.target_y_: y, lm.initial_h_: h}\n",
    "        cost, _, h = session.run([lm.train_loss_, lm.train_step_, lm.final_h_], feed_dict=feed_dict)\n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += batch_size * max_time\n",
    "\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" %\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ids are the questions, but are we passing both at once?\n",
    "# from what i can gather, this was built for one id at a time. will need to pass through both questions at once.\n",
    "reload(utils)\n",
    "def score_dataset(lm, session, Q1, Q2, y, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    # bi = utils.batch_generator(Q1, Q2, y, batch_size=50, max_time=100)\n",
    "    bi = batch_generator(Q1, Q2, y, max_time)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "\n",
    "#######\n",
    "\n",
    "learning_rate = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlstm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlstm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "assertion failed: [Expected shape for Tensor sequence_length:0 is ] [9000] [ but saw shape: ] [50]\n\t [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All, rnn/Assert/Assert/data_0, rnn/stack, rnn/Assert/Assert/data_2, rnn/Shape_1)]]\n\nCaused by op 'rnn/Assert/Assert', defined at:\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-71-b12d48fe6ea5>\", line 11, in <module>\n    lm.BuildCoreGraph()\n  File \"C:\\Users\\andre\\Documents\\UCB\\W266\\Project\\MyWorkingFiles\\rnnlstm.py\", line 73, in wrapper\n    return function(self, *args, **kwargs)\n  File \"C:\\Users\\andre\\Documents\\UCB\\W266\\Project\\MyWorkingFiles\\rnnlstm.py\", line 237, in BuildCoreGraph\n    sequence_length = self.ns_, inputs = self.embedded_layer_q1_ + self.embedded_layer_q2_)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 561, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 556, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 170, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 124, in Assert\n    condition, data, summarize, name=\"Assert\")\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 37, in _assert\n    summarize=summarize, name=name)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [9000] [ but saw shape: ] [50]\n\t [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All, rnn/Assert/Assert/data_0, rnn/stack, rnn/Assert/Assert/data_2, rnn/Shape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: assertion failed: [Expected shape for Tensor sequence_length:0 is ] [9000] [ but saw shape: ] [50]\n\t [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All, rnn/Assert/Assert/data_0, rnn/stack, rnn/Assert/Assert/data_2, rnn/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-b12d48fe6ea5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Run a training epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"[epoch %d] Completed in %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty_timedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt0_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-68-a40dcc961b80>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(lm, session, batch_iterator, train, verbose, tick_s, learning_rate)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_w_q1_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_w_q2_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_y_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_h_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtotal_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: assertion failed: [Expected shape for Tensor sequence_length:0 is ] [9000] [ but saw shape: ] [50]\n\t [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All, rnn/Assert/Assert/data_0, rnn/stack, rnn/Assert/Assert/data_2, rnn/Shape_1)]]\n\nCaused by op 'rnn/Assert/Assert', defined at:\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-71-b12d48fe6ea5>\", line 11, in <module>\n    lm.BuildCoreGraph()\n  File \"C:\\Users\\andre\\Documents\\UCB\\W266\\Project\\MyWorkingFiles\\rnnlstm.py\", line 73, in wrapper\n    return function(self, *args, **kwargs)\n  File \"C:\\Users\\andre\\Documents\\UCB\\W266\\Project\\MyWorkingFiles\\rnnlstm.py\", line 237, in BuildCoreGraph\n    sequence_length = self.ns_, inputs = self.embedded_layer_q1_ + self.embedded_layer_q2_)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 561, in dynamic_rnn\n    [_assert_has_shape(sequence_length, [batch_size])]):\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 556, in _assert_has_shape\n    packed_shape, \" but saw shape: \", x_shape])\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 170, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 124, in Assert\n    condition, data, summarize, name=\"Assert\")\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 37, in _assert\n    summarize=summarize, name=name)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): assertion failed: [Expected shape for Tensor sequence_length:0 is ] [9000] [ but saw shape: ] [50]\n\t [[Node: rnn/Assert/Assert = Assert[T=[DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](rnn/All, rnn/Assert/Assert/data_0, rnn/stack, rnn/Assert/Assert/data_2, rnn/Shape_1)]]\n"
     ]
    }
   ],
   "source": [
    "reload (rnnlstm)\n",
    "reload (utils)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlstm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        #bi = utils.batch_generator(Q1_train, Q2_train, y_train, batch_size, max_time)\n",
    "        bi = batch_generator(Q1_train, Q2_train, y_train, max_time)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi, train=True) \n",
    "\n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, Q1_test, Q2_test, y_test, name=\"Test set\")\n",
    "        print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# something is going on with pre-defining what is in the tensor, rather than passing as part of the batch i think...\n",
    "# need to keep looking"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
