{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project, Attempting to get RNN LSTM to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "import string          # various string functions\n",
    "import difflib         # classes and functions for comparing sequences\n",
    "import utils           # word processing functions and distance functions, pretty printing and data loading\n",
    "\n",
    "from importlib import reload       # reload external files\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# rnn code\n",
    "import rnnlstm\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import vocabulary, tf_embed_viz\n",
    "\n",
    "# set a default vocab size\n",
    "V=20000\n",
    "\n",
    "import plotly.plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# need to look into plotly issues with this line in python3\n",
    "# plotly.offline.init_notebook_mode() # run at the start of every ipython notebook\n",
    "\n",
    "from sklearn.metrics import log_loss    # used in measurement / scoring\n",
    "from sklearn.metrics import classification_report   # among other things, provides accuracy and f1\n",
    "\n",
    "# Your code\n",
    "# import rnnlm\n",
    "# import rnnlm_test\n",
    "# reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# Generate training data dataframe\n",
    "# train_lite for testing approach\n",
    "train = pd.read_csv('Data/train_lite.csv') #index_col=0\n",
    "# full train for when working\n",
    "# train = pd.read_csv('Data/train.csv') #index_col=0\n",
    "\n",
    "print (len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57674</td>\n",
       "      <td>5725</td>\n",
       "      <td>38477</td>\n",
       "      <td>How can I get a list of my Gmail accounts?</td>\n",
       "      <td>How do I find my list of GMail addresses?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185568</td>\n",
       "      <td>283254</td>\n",
       "      <td>283255</td>\n",
       "      <td>What are the symptoms of child abuse?</td>\n",
       "      <td>What are signs of child abuse?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>359111</td>\n",
       "      <td>400936</td>\n",
       "      <td>488715</td>\n",
       "      <td>What are some examples of selfishness?</td>\n",
       "      <td>Do we all occasionally tend to emotional masoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>204216</td>\n",
       "      <td>281934</td>\n",
       "      <td>307050</td>\n",
       "      <td>Is Qnet a scam?</td>\n",
       "      <td>Where is the registered office of Qnet in Mumbai?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5649</td>\n",
       "      <td>11103</td>\n",
       "      <td>11104</td>\n",
       "      <td>What do Americans think about Donald Trump?</td>\n",
       "      <td>What do you think about Donald Trump pick?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1    qid2                                    question1  \\\n",
       "0   57674    5725   38477   How can I get a list of my Gmail accounts?   \n",
       "1  185568  283254  283255        What are the symptoms of child abuse?   \n",
       "2  359111  400936  488715       What are some examples of selfishness?   \n",
       "3  204216  281934  307050                              Is Qnet a scam?   \n",
       "4    5649   11103   11104  What do Americans think about Donald Trump?   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0          How do I find my list of GMail addresses?             1  \n",
       "1                     What are signs of child abuse?             1  \n",
       "2  Do we all occasionally tend to emotional masoc...             0  \n",
       "3  Where is the registered office of Qnet in Mumbai?             0  \n",
       "4         What do you think about Donald Trump pick?             1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train['Unnamed: 0']\n",
    "del train['Unnamed: 0.1']\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload(vocabulary)\n",
    "reload(utils)\n",
    "question1 = train.question1.tolist()\n",
    "question2 = train.question2.tolist()\n",
    "is_duplicate = train.is_duplicate.tolist()\n",
    "\n",
    "# print('Question pairs: %d' % len(question1))\n",
    "\n",
    "# build word index\n",
    "questions = question1 + question2\n",
    "\n",
    "vocab = utils.build_vocab(questions, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(vocab.ordered_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "<class 'dict'>\n",
      "4319\n"
     ]
    }
   ],
   "source": [
    "# critical definition\n",
    "# maps all words to ID\n",
    "\n",
    "wordset =  vocab.word_to_id\n",
    "print (len(wordset))\n",
    "print (type(wordset))\n",
    "print (wordset.get('paying'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in index: 20000\n"
     ]
    }
   ],
   "source": [
    "word_index = vocab.ordered_words()\n",
    "\n",
    "print(\"Words in index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word_to_id.get('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_questions(questions, wordset, vocab):\n",
    "    sents = [\"<s>\" + s + \"</s>\" for s in questions]\n",
    "    # print (sents[0:5])\n",
    " \n",
    "    sequence = []\n",
    "    num_sequence = []\n",
    "    for s in sents:\n",
    "        # for each sentence in list of sentences\n",
    "        # print (s)\n",
    "        s_words = []\n",
    "        s_w_ids = []\n",
    "        for w in s.split():\n",
    "            # capture list of words for sentence\n",
    "            # once all captured, add to list with sequence\n",
    "            \n",
    "            # for each word in split sentence\n",
    "            # print (w)\n",
    "            w = w.lower()\n",
    "            if (wordset == None) or (w in wordset): \n",
    "                s_words.append(w)\n",
    "                s_w_ids.append(vocab.word_to_id.get(w))\n",
    "            else:\n",
    "                s_words.append(\"<unk>\") # unknown token\n",
    "                s_w_ids.append(vocab.word_to_id.get(\"<unk>\"))\n",
    "                               \n",
    "            '''\n",
    "            if w.isdigit():\n",
    "            if (wordset != None) and (w in wordset):\n",
    "                    q1_sequence.append(w)\n",
    "\n",
    "                w = utils.canonicalize_digits(w) # try to canonicalize numbers\n",
    "            '''\n",
    "\n",
    "        sequence.append(s_words)\n",
    "        num_sequence.append(s_w_ids)\n",
    "        \n",
    "        \n",
    "    sequence = np.array(sequence)\n",
    "    num_sequence = np.array(num_sequence)\n",
    "    return sequence, num_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_sequences, q1_seq_num = process_questions(question1, wordset, vocab)\n",
    "q2_sequences, q2_seq_num = process_questions(question2, wordset, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ ['<unk>', 'can', '<unk>', 'get', 'a', 'list', 'of', 'my', 'gmail', '<unk>'],\n",
       "       ['<unk>', 'are', 'the', 'symptoms', 'of', 'child', '<unk>'],\n",
       "       ['<unk>', 'are', 'some', 'examples', 'of', '<unk>'], ...,\n",
       "       ['<unk>', 'does', 'gases', 'heat', 'up', 'if', 'we', '<unk>', '<unk>', 'cool', 'it', 'by', 'expanding', 'it', 'above', 'inversion', '<unk>'],\n",
       "       ['<unk>', 'missed', 'the', 'call', 'on', 'brexit;', 'how', 'accurate', 'are', 'current', 'polling', 'methods', 'and', 'could', 'we', 'see', '<unk>', 'forecasting', 'flubs', 'with', 'the', 'presidential', '<unk>'],\n",
       "       ['<unk>', 'can', 'be', 'explained', 'about', 'the', 'hyperinflation', 'in', 'zimbabwe?', 'what', 'were', 'the', 'causes', 'of', 'it', 'and', 'its', 'impact', 'on', 'the', 'economy', 'of', 'the', '<unk>']], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 12217, 2, 19965, 17513, 9828, 555, 19375, 2],\n",
       "       [2, 4660, 2, 555, 7902, 2], [2, 1804, 1908, 10320, 283, 2, 4842, 2],\n",
       "       ...,\n",
       "       [2, 14700, 16579, 7584, 18066, 16042, 11744, 16681, 2, 18096, 12387, 2],\n",
       "       [2, 5579, 15937, 16315, 17422, 4330, 2, 2, 5311, 2649, 18991, 3081, 6831, 3225, 9854, 2, 2649, 19117, 9552, 18096, 12387, 2537, 17534, 18160, 398, 11205, 12369, 14717, 6831, 3306, 2, 2],\n",
       "       [2, 12217, 16550, 5546, 10249, 2649, 6887, 8637, 9552, 2]], dtype=object)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_seq_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len(q1_sequences))\n",
    "print (len(q2_sequences))\n",
    "\n",
    "# NOTE, with sequences, we no longer know one sentence from the next; they have now run together\n",
    "# not sure this is what we want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload (rnnlstm)\n",
    "\n",
    "# building graph based on a3 assigment\n",
    "TF_GRAPHDIR = \"tf_graph\"\n",
    "\n",
    "# Clear old log directory.\n",
    "shutil.rmtree(TF_GRAPHDIR, ignore_errors=True)\n",
    "\n",
    "# V, H, and num_layers are all things we can play with...\n",
    "# default corpus is 'brown'\n",
    "# for now, want to see running with this data\n",
    "# set hyperparameters\n",
    "lm = rnnlstm.RNNLM(V=V, H=200, num_layers=2)\n",
    "\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "summary_writer = tf.summary.FileWriter(TF_GRAPHDIR, lm.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.stack((q1_sequences, q2_sequences), axis=1)\n",
    "X = np.stack((q1_seq_num, q2_seq_num), axis=1)\n",
    "\n",
    "#y = is_duplicate\n",
    "y = np.array(is_duplicate)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4660, 18066, 555, 12369, 5846, 3116, 14686, 2]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(q1, q2, label):\n",
    "    for i in range(len(q1)):\n",
    "        # using a generator for return here\n",
    "        # Yield batches\n",
    "        yield q1[i], q2[i], label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing batch generator\n",
    "bi = batch_generator(Q1_train, Q2_train, y_train)\n",
    "\n",
    "# for i, (w1, w2, y) in enumerate(bi):\n",
    "#    print (\"i:\", i)\n",
    "#    print (\"w1:\", w1)\n",
    "#    print (\"w2:\", w2)\n",
    "#    print (\"y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "        \n",
    "    # How do I bring in both w's? Getting a \"need to pass W_1 through cost, _, h\" function. No idea.\n",
    "\n",
    "    for i, (w1, w2, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean initial state.\n",
    "\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_q1_: w1, lm.input_w_q2_: w2})\n",
    "\n",
    "       \n",
    "        feed_dict = {lm.input_w_q1_: w1, lm.input_w_q2_: w2, lm.target_y_: y, lm.initial_h_: h}\n",
    "        cost, _, h = session.run([lm.train_loss_, lm.train_step_, lm.final_h_], feed_dict=feed_dict)\n",
    "\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += batch_size * max_time\n",
    "\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print (\"[batch %d]: seen %d words at %d wps, loss = %.3f\" %\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ids are the questions, but are we passing both at once?\n",
    "# from what i can gather, this was built for one id at a time. will need to pass through both questions at once.\n",
    "reload(utils)\n",
    "def score_dataset(lm, session, Q1, Q2, y, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    # bi = utils.batch_generator(Q1, Q2, y, batch_size=50, max_time=100)\n",
    "    bi = batch_generator(Q1, Q2, y)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)\n",
    "    print (\"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# max_time = 100\n",
    "# not sure these matter any more:\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "\n",
    "#######\n",
    "\n",
    "learning_rate = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=vocab.size, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlstm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlstm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] Starting epoch 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (9,) for Tensor 'w1:0', which has shape '(?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-facf3d2ffee4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# Run a training epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"[epoch %d] Completed in %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretty_timedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt0_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-86-a40dcc961b80>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(lm, session, batch_iterator, train, verbose, tick_s, learning_rate)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_w_q1_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_w_q2_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andre\\Anaconda2\\envs\\Python3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    973\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m    976\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (9,) for Tensor 'w1:0', which has shape '(?, ?)'"
     ]
    }
   ],
   "source": [
    "reload (rnnlstm)\n",
    "reload (utils)\n",
    "\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlstm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        #bi = utils.batch_generator(Q1_train, Q2_train, y_train, batch_size, max_time)\n",
    "        bi = batch_generator(Q1_train, Q2_train, y_train)\n",
    "        print (\"[epoch %d] Starting epoch %d\" % (epoch, epoch))\n",
    "\n",
    "        # Run a training epoch.\n",
    "        \n",
    "        run_epoch(lm, session, bi, train=True) \n",
    "\n",
    "        print (\"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch)))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, Q1_test, Q2_test, y_test, name=\"Test set\")\n",
    "        print (\"\")\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
