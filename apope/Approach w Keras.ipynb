{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://github.com/bradleypallen/keras-quora-question-pairs/blob/master/keras-quora-question-pairs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import csv, datetime, time, json\n",
    "from zipfile import ZipFile\n",
    "from os.path import expanduser, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "#KERAS_DATASETS_DIR = expanduser('~/.keras/datasets/')\n",
    "KERAS_DATASETS_DIR = 'Data/'\n",
    "QUESTION_PAIRS_FILE_URL = 'http://qim.ec.quoracdn.net/quora_duplicate_questions.tsv'\n",
    "QUESTION_PAIRS_FILE = 'quora_duplicate_questions.tsv'\n",
    "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
    "GLOVE_FILE = 'glove.840B.300d.txt'\n",
    "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
    "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
    "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
    "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
    "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 300\n",
    "MODEL_WEIGHTS_FILE = 'question_pairs_weights.h5'\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "RNG_SEED = 13371447\n",
    "NB_EPOCHS = 25\n",
    "#NB_EPOCHS = 5\n",
    "DROPOUT = 0.1\n",
    "BATCH_SIZE = 32\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the dataset, embedding matrix and word count exist in the local directory\n",
    "if exists(Q1_TRAINING_DATA_FILE) and exists(Q2_TRAINING_DATA_FILE) and exists(LABEL_TRAINING_DATA_FILE) and exists(NB_WORDS_DATA_FILE) and exists(WORD_EMBEDDING_MATRIX_FILE):\n",
    "    # Then load them\n",
    "    q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
    "    q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
    "    labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
    "    word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
    "    with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
    "        nb_words = json.load(f)['nb_words']\n",
    "else:\n",
    "    # Else download and extract questions pairs data\n",
    "    if not exists(KERAS_DATASETS_DIR + QUESTION_PAIRS_FILE):\n",
    "        get_file(QUESTION_PAIRS_FILE, QUESTION_PAIRS_FILE_URL)\n",
    "\n",
    "    print(\"Processing\", QUESTION_PAIRS_FILE)\n",
    "\n",
    "    question1 = []\n",
    "    question2 = []\n",
    "    is_duplicate = []\n",
    "    with open(KERAS_DATASETS_DIR + QUESTION_PAIRS_FILE, encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            question1.append(row['question1'])\n",
    "            question2.append(row['question2'])\n",
    "            is_duplicate.append(row['is_duplicate'])\n",
    "\n",
    "    print('Question pairs: %d' % len(question1))\n",
    "\n",
    "    # Build tokenized word index\n",
    "    questions = question1 + question2\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(questions)\n",
    "    question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
    "    question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    print(\"Words in index: %d\" % len(word_index))\n",
    "\n",
    "    # Download and process GloVe embeddings\n",
    "    if not exists(KERAS_DATASETS_DIR + GLOVE_ZIP_FILE):\n",
    "        zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
    "        zipfile.extract(GLOVE_FILE, path=KERAS_DATASETS_DIR)\n",
    "\n",
    "    print(\"Processing\", GLOVE_FILE)\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with open(KERAS_DATASETS_DIR + GLOVE_FILE, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "\n",
    "    print('Word embeddings: %d' % len(embeddings_index))\n",
    "\n",
    "    # Prepare word embedding matrix\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            word_embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "\n",
    "    # Prepare training data tensors\n",
    "    q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    labels = np.array(is_duplicate, dtype=int)\n",
    "    print('Shape of question1 data tensor:', q1_data.shape)\n",
    "    print('Shape of question2 data tensor:', q2_data.shape)\n",
    "    print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "    # Persist training and configuration data to files\n",
    "    np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), q1_data)\n",
    "    np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), q2_data)\n",
    "    np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
    "    np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
    "    with open(NB_WORDS_DATA_FILE, 'w') as f:\n",
    "        json.dump({'nb_words': nb_words}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (95597, 300)\n",
      "Null word embeddings: 29276\n",
      "Shape of question1 data tensor: (404290, 25)\n",
      "Shape of question2 data tensor: (404290, 25)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "#print('Word embeddings: ' % word_embedding_matrix.shape)\n",
    "print('Shape of embedding matrix:', word_embedding_matrix.shape)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_embedding_matrix[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q1_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partition the dataset into train and test sets\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
    "q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
    "q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
    "\n",
    "merged = concatenate([q1,q2])\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='relu')(merged)\n",
    "merged = Dropout(DROPOUT)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training at 2017-08-15 00:07:55.578227\n",
      "Train on 327474 samples, validate on 36387 samples\n",
      "Epoch 1/25\n",
      "542s - loss: 0.5391 - acc: 0.7276 - val_loss: 0.4932 - val_acc: 0.7544\n",
      "Epoch 2/25\n",
      "536s - loss: 0.4876 - acc: 0.7606 - val_loss: 0.4723 - val_acc: 0.7721\n",
      "Epoch 3/25\n",
      "537s - loss: 0.4623 - acc: 0.7768 - val_loss: 0.4478 - val_acc: 0.7814\n",
      "Epoch 4/25\n",
      "543s - loss: 0.4430 - acc: 0.7887 - val_loss: 0.4445 - val_acc: 0.7811\n",
      "Epoch 5/25\n",
      "538s - loss: 0.4229 - acc: 0.8004 - val_loss: 0.4342 - val_acc: 0.7893\n",
      "Epoch 6/25\n",
      "536s - loss: 0.4085 - acc: 0.8099 - val_loss: 0.4299 - val_acc: 0.7943\n",
      "Epoch 7/25\n",
      "535s - loss: 0.3934 - acc: 0.8189 - val_loss: 0.4284 - val_acc: 0.7916\n",
      "Epoch 8/25\n",
      "536s - loss: 0.3813 - acc: 0.8260 - val_loss: 0.4135 - val_acc: 0.8027\n",
      "Epoch 9/25\n",
      "534s - loss: 0.3712 - acc: 0.8320 - val_loss: 0.4160 - val_acc: 0.8008\n",
      "Epoch 10/25\n",
      "533s - loss: 0.3602 - acc: 0.8389 - val_loss: 0.4223 - val_acc: 0.7974\n",
      "Epoch 11/25\n",
      "536s - loss: 0.3518 - acc: 0.8442 - val_loss: 0.4148 - val_acc: 0.8044\n",
      "Epoch 12/25\n",
      "536s - loss: 0.3411 - acc: 0.8490 - val_loss: 0.4095 - val_acc: 0.8083\n",
      "Epoch 13/25\n",
      "533s - loss: 0.3332 - acc: 0.8536 - val_loss: 0.4147 - val_acc: 0.8060\n",
      "Epoch 14/25\n",
      "536s - loss: 0.3264 - acc: 0.8572 - val_loss: 0.4079 - val_acc: 0.8108\n",
      "Epoch 15/25\n",
      "533s - loss: 0.3172 - acc: 0.8631 - val_loss: 0.4092 - val_acc: 0.8085\n",
      "Epoch 16/25\n",
      "533s - loss: 0.3105 - acc: 0.8657 - val_loss: 0.4153 - val_acc: 0.8101\n",
      "Epoch 17/25\n",
      "533s - loss: 0.3063 - acc: 0.8682 - val_loss: 0.4512 - val_acc: 0.7911\n",
      "Epoch 18/25\n",
      "533s - loss: 0.3036 - acc: 0.8696 - val_loss: 0.4185 - val_acc: 0.8086\n",
      "Epoch 19/25\n",
      "534s - loss: 0.2969 - acc: 0.8730 - val_loss: 0.4280 - val_acc: 0.8028\n",
      "Epoch 20/25\n",
      "533s - loss: 0.2938 - acc: 0.8744 - val_loss: 0.4143 - val_acc: 0.8077\n",
      "Epoch 21/25\n",
      "533s - loss: 0.2874 - acc: 0.8782 - val_loss: 0.4590 - val_acc: 0.7965\n",
      "Epoch 22/25\n",
      "534s - loss: 0.2830 - acc: 0.8799 - val_loss: 0.4288 - val_acc: 0.8079\n",
      "Epoch 23/25\n",
      "533s - loss: 0.2791 - acc: 0.8814 - val_loss: 0.4325 - val_acc: 0.8103\n",
      "Epoch 24/25\n",
      "534s - loss: 0.2758 - acc: 0.8834 - val_loss: 0.4386 - val_acc: 0.8047\n",
      "Epoch 25/25\n",
      "533s - loss: 0.2696 - acc: 0.8858 - val_loss: 0.4337 - val_acc: 0.8086\n",
      "Training ended at 2017-08-15 03:51:08.272810\n",
      "Minutes elapsed: 223.211568\n"
     ]
    }
   ],
   "source": [
    "# Train the model, checkpointing weights with best validation accuracy\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=NB_EPOCHS,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
